{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # LIBRARIES NEEDED FOR THE PROGRAM # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General libraries, twitter and mongodb libraries\n",
    "import tweepy as tpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "#Text preprocessing libraries\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "\n",
    "#Vectorizing and machine learning algorith libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, svm\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Library for Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Library for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Library for WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Libraries for Analytics\n",
    "from numpy.random import seed, randn\n",
    "from scipy.stats import ttest_ind, stats, f_oneway, chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Gathering Data From Twitter API & Coverting to DataFrame # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to generate twitter API\n",
    "def create_api():\n",
    "    \n",
    "    #Getting input from the user like:\n",
    "    #Twitter Consumer Key\n",
    "    consumer_key = input(\"Enter Consumer Key: \")\n",
    "    #Twitter Consumer Secret Key\n",
    "    consumer_secret = input(\"Enter Consumer Secret: \")\n",
    "    #Twitter Access Token\n",
    "    access_token = input(\"Enter Access Token: \")\n",
    "    #Twitter Access Token Secret\n",
    "    access_secret = input(\"Enter Access Secret: \")\n",
    "        \n",
    "    #Authenticating the twitter consumer key and secret using tweepy library\n",
    "    authenticate = tpy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    #Authenticating the twitter access token and secret using tweepy library\n",
    "    authenticate.set_access_token(access_token, access_secret)\n",
    "    \n",
    "    #Creating the twitter API using tweepy library\n",
    "    api = tpy.API(authenticate)\n",
    "    \n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = create_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract twitter data\n",
    "def twitter_extraction(api):\n",
    "    \n",
    "    #Getting the clinet link for MongoDB connection from the user\n",
    "    mongo_client_link = input(\"Please enter the connectivity link: \")\n",
    "    \n",
    "    #creating the connection with MongoDB using the pymongo library\n",
    "    #passing the information to the object\n",
    "    client = pymongo.MongoClient(mongo_client_link)\n",
    "    #creating the database in MongoDB to store the raw data extracted from twitter API\n",
    "    raw_tweet_db = client['raw_tweet_db']\n",
    "    #creating the collection to store the extracted information inside the database\n",
    "    raw_tweet_collection = raw_tweet_db['raw_tweet_collection']\n",
    "    \n",
    "    #Getting the search string from the user\n",
    "    tweet_search = input(\"Please enter the tweet you want to retrieve: \")\n",
    "    \n",
    "    #Getting the total number of records that you want retrive from twitter\n",
    "    tweet_max = int(input(\"Please enter how many records you want to retrieve: \"))\n",
    "    \n",
    "    #Getting the category name for an additional column in the database\n",
    "    category_name = input(\"Please enter category name: \")\n",
    "    \n",
    "    #Loop where it is retrieving the data from twitter and parsing through each records to save it in MongoDB\n",
    "    for tweet in tpy.Cursor(api.search, q = tweet_search, lang = 'en',\n",
    "                            exclude='retweets', tweet_mode = 'extended').items(tweet_max):\n",
    "        \n",
    "        #converting the retrieve data into dictionary format and storing into variable\n",
    "        raw_tweets = dict(tweet._json)\n",
    "        \n",
    "        indx = list(raw_tweets.values())[1]\n",
    "        \n",
    "        print(json.dumps(raw_tweets, indent = 3))\n",
    "        \n",
    "        #inserting each record retrieved from the Twitter API in MongoDB database\n",
    "        raw_tweet_collection.insert_one(raw_tweets)\n",
    "        #adding a new column in the extsting database for category\n",
    "        raw_tweet_collection.update_many({'id': indx},{\"$set\": {'category_name': category_name}})\n",
    "        \n",
    "    return raw_tweet_collection, mongo_client_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc, mcl = twitter_extraction(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selection(mcl, raw_tweet_collection):\n",
    "    \n",
    "    #creating the connection with MongoDB using the pymongo library\n",
    "    #passing the information to the object\n",
    "    client = pymongo.MongoClient(mcl)\n",
    "    \n",
    "    #Loop to retrieve selected columns from the raw data stored inside the database\n",
    "    query = raw_tweet_collection.find({},{'_id':0, 'created_at':1, 'id':1, 'full_text':1, \n",
    "                                    'entities.hashtags.text':1, 'entities.user_mentions.screen_name':1, \n",
    "                                    'entities.user_mentions.id':1, 'user.id':1, 'user.name':1 , 'user.screen_name':1, \n",
    "                                    'user.location':1, 'user.protected':1, 'user.followers_count':1, 'user.friends_count':1, \n",
    "                                    'user.listed_count':1, 'user.created_at':1, 'user.favourites_count':1, 'user.statuses_count':1, \n",
    "                                    'retweeted_status.created_at':1, 'retweeted_status.id':1, 'retweeted_status.full_text':1,\n",
    "                                    'retweeted_status.user_mentions.screen_name':1, \n",
    "                                    'retweet_count':1, 'favorite_count':1, \n",
    "                                    'possibly_sensitive':1, 'lang':1, 'category_name':1})\n",
    "    \n",
    "    #creating a new database in MongoDB to store the selected data extracted from MongoDB\n",
    "    selected_tweet_columns_db = client['selected_tweet_columns_db']\n",
    "    \n",
    "    #creating the collection to store the selected information inside new collection\n",
    "    selected_tweet_columns_collection = selected_tweet_columns_db['selected_tweet_columns_collection']\n",
    "    \n",
    "    #Loop to parse through the selected data and store inside new collection\n",
    "    for q in query:\n",
    "        \n",
    "        #printing the selected data\n",
    "        print(json.dumps(q, indent = 3))\n",
    "        \n",
    "        #inserting the selected tweets inside new collection in MongoDB Database\n",
    "        selected_tweet_columns_collection.insert_one(q)\n",
    "    \n",
    "    return selected_tweet_columns_db, selected_tweet_columns_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stcd, stcc = data_selection(mcl, rtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the data to pandas dataframe \n",
    "def load_data(selected_tweet_columns_db, selected_tweet_columns_collection, mongo_client_link):\n",
    "    \n",
    "    #creating the connection with MongoDB using the pymongo library\n",
    "    #passing the information to the object\n",
    "    client = pymongo.MongoClient(mongo_client_link)\n",
    "    \n",
    "    #assigining MongoDB database to a variable\n",
    "    #mongo_db = selected_tweet_columns_db\n",
    "    mongo_db = client['selected_tweet_columns_db']\n",
    "    \n",
    "    #assigning MongoDB database collection to a variable\n",
    "    #collection = mongo_db.selected_tweet_columns_collection\n",
    "    collection = mongo_db['selected_tweet_columns_collection']\n",
    "    \n",
    "    tweets_df = pd.json_normalize(collection.find({},{'_id':0}), max_level=2)\n",
    "    \n",
    "    #dropping all the duplicate rows from\n",
    "    rd_tweets_df = tweets_df.drop_duplicates(subset = ['full_text'])\n",
    "    \n",
    "    #Extracting only the tweets from the exisisting dataframe and creating a new dataframe\n",
    "    data = [rd_tweets_df['full_text'], rd_tweets_df['category_name']]\n",
    "    header = ['content', 'category_name']\n",
    "    new_tweet_df = pd.concat(data, axis = 1, keys = header)\n",
    "    \n",
    "    location = input(\"Please enter the path where you want to save the file: \")\n",
    "    \n",
    "    fname = input(\"Please enter the file name: \")\n",
    "    \n",
    "    #saving the dataframe as csv\n",
    "    rd_tweets_df.to_csv(location+fname)\n",
    "    \n",
    "    return rd_tweets_df, new_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet_df, modified_tweet_df = load_data(stcd, stcc, mcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Training Data Import from MongoDB # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainingData(mongo_client_link):\n",
    "#Making the connection between python and MongoDB\n",
    "\n",
    "    client = pymongo.MongoClient(mongo_client_link)\n",
    "\n",
    "    #Connecting to datbase and collection of MongoDB\n",
    "    tclass = client['troll_classification']\n",
    "    tcollec = tclass['collection_troll']\n",
    "\n",
    "    #printing the connection to the collection from MongoDB\n",
    "    print(tcollec)\n",
    "    \n",
    "    #Loading the records to pandas dataframe excluding the auto-generated id by MongoDB\n",
    "    training_df = pd.json_normalize(tcollec.find({},{'_id':0, 'extras':0, 'annotation.notes': 0 }))\n",
    "    \n",
    "    #creating an empty list to store the troll label that we would be extracted from the array \n",
    "    troll_label = []\n",
    "\n",
    "    for i in training_df['annotation.label']:\n",
    "        for j in i:\n",
    "            troll_label.append(j)\n",
    "    training_df['Troll_label'] = troll_label\n",
    "    \n",
    "    training_df = training_df.drop(labels = 'annotation.label', axis=1)\n",
    "    \n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = load_trainingData(mcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # PreProcessing of Text Data (Tweets) - NLP PreProcessing # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict={\"dunno\": \"do not know\", \"wanna\": \"want to\", \"what's\":\"what is\", \"what're\":\"what are\",\"who's\":\"who is\",\"who're\":\"who are\",\"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\"when's\":\"when is\",\"when're\":\"when are\",\"how's\":\"how is\",\"how're\":\"how are\",\n",
    "    \"i'm\":\"i am\",\"we're\":\"we are\",\"you're\":\"you are\",\"they're\":\"they are\",\"it's\":\"it is\",\"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\"that's\":\"that is\",\"there's\":\"there is\",\"there're\":\"there are\",\"i've\":\"i have\",\"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\"they've\":\"they have\",\"who've\":\"who have\",\"would've\":\"would have\",\"not've\":\"not have\",\n",
    "    \"i'll\":\"i will\",\"we'll\":\"we will\",\"you'll\":\"you will\",\"he'll\":\"he will\",\"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\"they'll\":\"they will\",\"I'll\":\"i will\",\"isn't\":\"is not\",\"wasn't\":\"was not\",\"aren't\":\"are not\",\"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\"couldn't\":\"could not\",\"don't\":\"do not\",\"didn't\":\"did not\",\"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\"doesn't\":\"does not\",\"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\"u\":\"you\",\"ur\":\"your\", \"rolf\": \"rolling on floor laughing\", \"stfu\": \"shut the fuck up\", \n",
    "    \"icymi\": \"in case you missed it\", \"tl;dr\": \"too long, didn’t read\", \"lmk\": \"let me know\",\"nvm\": \"nevermind\",\n",
    "    \"tgif\": \"thank goodness it’s Friday\", \"tbh\": \"to be honest\", \"tbf\": \"to be frank\", \"rn\": \"right now\",\n",
    "    \"qotd\": \"quote of the day\", \"brb\": \"be right back\", \"btw\": \"by the way\", \"lol\": \"laugh out loud\", \n",
    "    \"ttyl\": \"talk to you later\", \"hmu\": \"hit me up\", \"fwiw\": \"for what it’s worth\",\n",
    "    \"imo\": \"in my opinion\", \"imho\": \"in my humble opinion\", \"idk\": \"i do not know\", \"tba\": \"to be announced\",\n",
    "    \"tbd\": \"to be decided\", \"faq\": \"frequently asked question\", \"asap\": \"as soon as possible\", \n",
    "    \"aka\": \"also known as\", \"diy\": \"do it yourself\", \"np\": \"o problem\", \"ty\": \"thank you\", \"hifw\": \"how i feel when\",\n",
    "    \"bts\": \"behind the scenes\", \"cmv\": \"change my view\", \"dyk\": \"did you know\", \"eli5\" : \"explain it to me like i am five\",\n",
    "    \"ftw\": \"for the win\", \"irl\": \"in real life\", \"nbd\": \"no big deal\", \"oc\": \"original content\", \"tftf\": \"thanks for the follow\",\n",
    "    \"tfw\": \"that feeling when\", \"tigf\": \"thank god it is friday\", \"f*ck\": \"fuck\", \"f***k\": \"fuck\", \"s**k\": \"suck\",\n",
    "    \"b***h\": \"bitch\", \"b**ch\": \"bitch\", \"a**\": \"ass\", \"a**h*le\": \"asshole\", \"fu*k\": \"fuck\", \"sh*t\": \"shit\", \"s**t\": \"shit\",\n",
    "    \"omg\": \"oh my god\", \"ily\": \"i love you\", \"lmao\": \"laughing my ass off\", \"wtf\": \"what the fuck\", \"ppl\": \"people\",\n",
    "    \"thx\": \"thanks\", \"ffs\": \"for fuck's sake\", \"fml\": \"fuck my life\", \"stfu\": \"shut the fuck up\", \"jj\": \"just joking\",\n",
    "    \"jk\": \"just kidding\", \"bff\": \"best friend forever\", \"ftw\": \"for the win\", \"txt\": \"text\", \"hbd\": \"happy birthday\",\n",
    "    \"gtfo\": \"get the fuck out\", \"dgaf\": \"do not give a fuck\", \"dtf\": \"down to fuck\", \"smfh\": \"shaking my fucking head\",\n",
    "    \"roflmao\": \"rolling on floor laughing my ass off\", \"ptfo\": \"passed the fuck out\", \"ttys\": \"talk to you soon\",\n",
    "    \"fbo\": \"facebook official\", \"ttyn\": \"talk to you never\", \"b4\": \"before\", \"bae\": \"before anyone else\", \"btaim\": \"be that as it may\",\n",
    "    \"cx\": \"customer experience\", \"dm\": \"direct message\", \"f2f\": \"face to face\", \"b2b\": \"business to business\",\n",
    "    \"b2c\": \"business to customer\", \"fb\": \"facebook\", \"ftfy\": \"fixed that for you\", \"g2g\": \"got to go\", \"gr8\": \"great\",\n",
    "    \"hmb\": \"hit me back\", \"hmu\": \"hit me up\", \"hth\": \"happy to help\", \"ianad\": \"i am not a doctor\", \"ianal\": \"i am not a lawyer\",\n",
    "    \"idc\": \"i do not care\", \"ig\": \"instagram\", \"rss\": \"really simple syndication\", \"rt\": \"retweet\", \"motherf**ker\": \"motherfucker\",\n",
    "    \"motherfu*cker\": \"motherfucker\", \"'em\": \"them\", \"ik\": \"i know\", \"what&;s\":\"what is\", \"what&;re\":\"what are\",\n",
    "    \"who&;s\":\"who is\",\"who&;re\":\"who are\",\"where&;s\":\"where is\",\n",
    "    \"where&;re\":\"where are\",\"when&;s\":\"when is\",\"when&;re\":\"when are\",\"how&;s\":\"how is\",\"how&;re\":\"how are\",\n",
    "    \"i&;m\":\"i am\",\"we&;re\":\"we are\",\"you&;re\":\"you are\",\"they&;re\":\"they are\",\"it&;s\":\"it is\",\"he&;s\":\"he is\",\n",
    "    \"she&;s\":\"she is\",\"that&;s\":\"that is\",\"there&;s\":\"there is\",\"there&;re\":\"there are\",\"i&;ve\":\"i have\",\"we&;ve\":\"we have\",\n",
    "    \"you&;ve\":\"you have\",\"they&;ve\":\"they have\",\"who&;ve\":\"who have\",\"would&;ve\":\"would have\",\"not&;ve\":\"not have\",\n",
    "    \"i&;ll\":\"i will\",\"we&;ll\":\"we will\",\"you&;ll\":\"you will\",\"he&;ll\":\"he will\",\"she&;ll\":\"she will\",\n",
    "    \"it&;ll\":\"it will\",\"they&;ll\":\"they will\",\"I&;ll\":\"i will\",\"isn&;t\":\"is not\",\"wasn&;t\":\"was not\",\n",
    "    \"aren&;t\":\"are not\",\"weren&;t\":\"were not\",\n",
    "    \"can&;t\":\"can not\",\"couldn&;t\":\"could not\",\"don&;t\":\"do not\",\"didn&;t\":\"did not\",\"shouldn&;t\":\"should not\",\n",
    "    \"wouldn&;t\":\"would not\",\"doesn&;t\":\"does not\",\"haven&;t\":\"have not\",\"hasn&;t\":\"has not\",\"hadn&;t\":\"had not\",\n",
    "    \"won&;t\":\"will not\", \"gonna\": \"got to\", \"gotcha\": \"i have got you\", \"d\": \"the\", \"n\": \"and\", \"amp\": \"and\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to preprocess the text data\n",
    "def nlp_preprocessing(dataframe):\n",
    "    \n",
    "    #Removing of URL, Mentions, Hastages, Reserved Words (RT and FAV), Emoji, Smiley and Number\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: \" \".join(p.clean(x) for x in x.split()))\n",
    "    \n",
    "    #converting the contents in lower case\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    #normalizing short words\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: ' '.join([abbr_dict[x] if x in abbr_dict else x for x in x.split()]))\n",
    "    \n",
    "    #removing any character which is not alphabets from the string\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\n",
    "    \n",
    "    #removing the stopwords from the contents\n",
    "    stop = stopwords.words('english')\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    \n",
    "    #lemmatization of the words from the content\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training_df = nlp_preprocessing(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Splitting Data for Training and Testing and Vectorization# # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe):\n",
    "    \n",
    "    #randomizing the dataframe contents\n",
    "    dataframe = dataframe.sample(frac = 1)\n",
    "    \n",
    "    #splitting the data into train and test\n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(dataframe['content'], \n",
    "                                                                          dataframe['Troll_label'])\n",
    "    \n",
    "    return train_x, valid_x, train_y, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(clean_training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vectorizing(dataframe, train_x, valid_x):\n",
    "    \n",
    "    #Vectorizing the data using TF-IDF vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect.fit(dataframe['content'])\n",
    "    #transforming the training and testing data\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "    \n",
    "    return xtrain_tfidf, xvalid_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tfidf, xvalid_tfidf = data_vectorizing(clean_training_df, train_x, valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newdata_vectorizing(dataframe):\n",
    "    \n",
    "    #Vectorizing the data using TF-IDF vectorizer\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect.fit(dataframe['content'])\n",
    "    #transforming the dataframe\n",
    "    tweet_tfidf =  tfidf_vect.transform(dataframe['content'])\n",
    "    \n",
    "    return tweet_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Model Training and Testing # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model_train_test(xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    multinomial = Pipeline([('m', naive_bayes.MultinomialNB(alpha=0.2))])\n",
    "    model.append(('multinomial', multinomial))\n",
    "    \n",
    "    decision_tree = Pipeline([('m', DecisionTreeClassifier(random_state = 2))])\n",
    "    model.append(('decisiontree', decision_tree))\n",
    "    \n",
    "    random_forest = Pipeline([('m', RandomForestClassifier())])\n",
    "    model.append(('randomforest', random_forest))\n",
    "    \n",
    "    svc = Pipeline([('m', svm.SVC())])\n",
    "    model.append(('svc', svc))\n",
    "    \n",
    "    passive_aggressive = Pipeline([('m', linear_model.PassiveAggressiveClassifier(C = 0.5, random_state = 5))])\n",
    "    model.append(('passiveaggressive', passive_aggressive))\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators = model, voting = 'hard')\n",
    "    \n",
    "    ensemble_fit = ensemble.fit(xtrain, ytrain)\n",
    "    \n",
    "    predictions = ensemble.predict(xtest)\n",
    "    \n",
    "    print(\"Prediction: \", predictions)\n",
    "    \n",
    "    print(\"Model Accuracy: \", accuracy_score(ytest, predictions)*100)\n",
    "     \n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, predictions)) \n",
    "    \n",
    "    return ensemble, ensemble_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble, ensemble_fit = custom_model_train_test(xtrain_tfidf, train_y, xvalid_tfidf, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing new dataset extracted from twitter\n",
    "\n",
    "def new_data_test(ensemble, ensemble_fit, xtest):\n",
    "    \n",
    "    #model fit values\n",
    "    ensemble_fit\n",
    "    \n",
    "    #predicting if the tweet is troll or not\n",
    "    predictions = ensemble.predict(xtest)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Sentiment Analysis # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(dataframe):\n",
    "    \n",
    "    #SentimentIntensityAnalyzer initialization\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    #creating an empty list to store the sentiment of the tweets\n",
    "    sentiment_val = []\n",
    "    polarity_score = []\n",
    "    #extracting the tweets from the content column of the dataframe \n",
    "    for senti in dataframe['content']:\n",
    "        \n",
    "        #passing the tweets to the model to get the polarity score of the tweet\n",
    "        sentiment_dict = analyser.polarity_scores(senti)   \n",
    "        polarity_score.append(sentiment_dict['compound'])\n",
    "        \n",
    "        # # decide sentiment as positive, negative and neutral on the basis of compound score\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            val = 'Positive'\n",
    "            sentiment_val.append(val)\n",
    "\n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            val = 'Negative'\n",
    "            sentiment_val.append(val)\n",
    "\n",
    "        else :\n",
    "            val = 'Neutral'\n",
    "            sentiment_val.append(val)\n",
    "    \n",
    "    return sentiment_val, polarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Working with Tweets # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_clean_tweet_df = nlp_preprocessing(modified_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tfidf = newdata_vectorizing(modified_clean_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_data_test(ensemble, ensemble_fit, tweet_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_val, sentiment_polarity = sentiment_analysis(modified_clean_tweet_df)\n",
    "print(\"Sentiment Value: \", sentiment_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSentiment Polarity: \", sentiment_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tweet_df['Troll_label'] = predictions\n",
    "original_tweet_df['Text_Sentiment'] = sentiment_val\n",
    "original_tweet_df['Sentiment_Polarity'] = sentiment_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_tweet_df.to_csv(\"Tweets_classified.csv\")\n",
    "list(original_tweet_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_not_stopwords_barplot(dataframe):\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    text = dataframe['content'].str.split()\n",
    "    text = text.values.tolist()\n",
    "    corpus = [word for i in text for word in i]\n",
    "\n",
    "    counter = Counter(corpus)\n",
    "    most = counter.most_common()\n",
    "    \n",
    "    words, counts = [], []\n",
    "    \n",
    "    for w,c in most[:10]:\n",
    "        if (w not in stop):\n",
    "            words.append(w)\n",
    "            counts.append(c)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "            \n",
    "    sns.barplot(x=counts,y=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_not_stopwords_barplot(modified_clean_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_number_histogram(dataframe):\n",
    "     dataframe['content'].str.split().\\\n",
    "        map(lambda x: len(x)).\\\n",
    "        hist(figsize = (8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_number_histogram(modified_clean_tweet_df)\n",
    "#It is clear that the number of words in tweets ranges from 0 to 35 \n",
    "#and mostly falls between 0 to 11 words as per the below chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_stopwords_barchart(dataframe):\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    \n",
    "    text = dataframe['full_text'].str.split()\n",
    "    text = text.values.tolist()\n",
    "    corpus = [word for i in text for word in i]\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    for word in corpus:\n",
    "        if word in stopword:\n",
    "            dic[word]+=1\n",
    "            \n",
    "    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    \n",
    "    x,y=zip(*top)\n",
    "    \n",
    "    plt.subplots(figsize = (8,8))\n",
    "    \n",
    "    plt.bar(x,y)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stopwords_barchart(original_tweet_df)\n",
    "\n",
    "#As we can see from the below barplot that the most frequent 10 stopwords used are:\n",
    "#\"the, to, and, of, a, in, is, for, on and with\" in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe to see the total number of category available \n",
    "# the total number of tweets in each category\n",
    "\n",
    "trct = original_tweet_df['category_name'].value_counts()\n",
    "\n",
    "category_name = original_tweet_df['category_name'].unique()\n",
    "\n",
    "records = []\n",
    "\n",
    "for count in trct:\n",
    "    records.append(count)\n",
    "\n",
    "records\n",
    "\n",
    "data = {'category': category_name, 'Total_Tweets': records}\n",
    "\n",
    "trct_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Total number of records in each category of tweets:\\n\\n\", trct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting pie chart from the above dataframe\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize= (8,8))\n",
    "ax1.pie(trct_df['Total_Tweets'], labels = trct_df['category'], autopct = '%1.1f%%', shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trolls and Not Trolls tweets from each category\n",
    "\n",
    "newDf = pd.DataFrame(original_tweet_df, columns=['category_name', 'Troll_label'])\n",
    "\n",
    "a = newDf.groupby(['category_name', 'Troll_label'])[['Troll_label']].count()\n",
    "\n",
    "tl=[]\n",
    "not_troll=[]\n",
    "troll=[]\n",
    "for x in a['Troll_label']:\n",
    "    tl.append(x)\n",
    "for y in tl:\n",
    "    if tl.index(y)%2 == 0:\n",
    "        not_troll.append(y)\n",
    "    else:\n",
    "        troll.append(y)\n",
    "\n",
    "info = {'category' : category_name, 'Not Troll': not_troll, 'Troll' : troll}\n",
    "\n",
    "tntcc = pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tntcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(tntcc['category']))\n",
    "\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "rects1 = ax.bar(x - width/2, tntcc['Not Troll'], width, label='Not Troll')\n",
    "rects2 = ax.bar(x + width/2, tntcc['Troll'], width, label='Troll')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Troll and Not Troll by Category')\n",
    "plt.xticks(x, rotation=90)\n",
    "ax.set_xticklabels(tntcc['category'])\n",
    "ax.legend(fontsize = 10)\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 10),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(dataframe):\n",
    "    comment_words = ''\n",
    "    \n",
    "    cat_name = input(\"Please enter category name: \")\n",
    "\n",
    "    text = dataframe.loc[dataframe['category_name'] == cat_name, 'full_text']\n",
    "    \n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, \n",
    "                  p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "    text = text.apply(lambda x: \" \".join(p.clean(x) for x in x.split()))\n",
    "    \n",
    "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))\n",
    "    \n",
    "    text = text.apply(lambda x: ' '.join([abbr_dict[x] if x in abbr_dict else x for x in x.split()]))\n",
    "    \n",
    "    content = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    \n",
    "    # iterate through the csv file\n",
    "    for val in content:\n",
    "\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "        stopwords = set(STOPWORDS)\n",
    "        \n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color ='white',\n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image                       \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_cloud(original_tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage calculation of Troll and Not Troll tweets\n",
    "tntcc['Troll_percentage'] = ((tntcc['Troll'] / (tntcc['Troll'] + tntcc['Not Troll']))*100).round(decimals = 2)\n",
    "tntcc['Not_Troll_percentage'] = ((tntcc['Not Troll'] / (tntcc['Troll'] + tntcc['Not Troll']))*100).round(decimals = 2)\n",
    "tntcc.to_csv(\"troll_nottroll.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe of category_name, troll_label and reteet_count, followers_count\n",
    "newDf1 = pd.DataFrame(original_tweet_df, \n",
    "                      columns=['category_name', 'Troll_label', 'retweet_count', 'user.followers_count',\n",
    "                              'user.favourites_count', 'user.friends_count',\n",
    "                               'user.statuses_count', 'favorite_count', 'Text_Sentiment', 'Sentiment_Polarity'])\n",
    "newDf1['Troll_label'] = newDf1['Troll_label'].astype(int)\n",
    "newDf1['retweet_count'] = newDf1['retweet_count'].astype(int)\n",
    "newDf1['user.followers_count'] = newDf1['user.followers_count'].astype(int)\n",
    "newDf1['user.favourites_count'] = newDf1['user.favourites_count'].astype(int)\n",
    "newDf1['user.friends_count'] = newDf1['user.friends_count'].astype(int)\n",
    "newDf1['user.followers_count'] = newDf1['user.followers_count'].astype(int)\n",
    "newDf1['user.statuses_count'] = newDf1['user.statuses_count'].astype(int)\n",
    "newDf1['favorite_count'] = newDf1['favorite_count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the statistical description of the dataset\n",
    "newDf1.describe().round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of co-relation of each category\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(newDf1.drop(\"Troll_label\", axis=1).corr(), annot = True, vmin=-1, vmax=1, center= 2, cmap= 'rainbow', linewidths=2, linecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Troll and Not Troll on the basis of Sentiment\n",
    "\n",
    "ntts = original_tweet_df.loc[original_tweet_df['Troll_label'] == '0', 'Text_Sentiment']\n",
    "tts = original_tweet_df.loc[original_tweet_df['Troll_label'] == '1', 'Text_Sentiment']\n",
    "\n",
    "nt_pos, nt_neu, nt_neg, t_pos, t_neu, t_neg = [], [], [], [], [], []\n",
    "\n",
    "Total_Not_Troll = tntcc['Not Troll'].sum()\n",
    "Total_Troll = tntcc['Troll'].sum()\n",
    "\n",
    "for i in ntts:\n",
    "    if (i == 'Positive'):\n",
    "        nt_pos.append(i)\n",
    "    elif (i == 'Negative'):\n",
    "        nt_neg.append(i)\n",
    "    else:\n",
    "        nt_neu.append(i)\n",
    "\n",
    "for j in tts:\n",
    "    if (j == 'Positive'):\n",
    "        t_pos.append(j)\n",
    "    elif (j == 'Negative'):\n",
    "        t_neg.append(j)\n",
    "    else:\n",
    "        t_neu.append(j)\n",
    "\n",
    "Troll_Pos = len(t_pos)\n",
    "Troll_Neg = len(t_neg)\n",
    "Troll_Neu = len(t_neu)\n",
    "NTroll_Pos = len(nt_pos)\n",
    "NTroll_Neg = len(nt_neg)\n",
    "NTroll_Neu = len(nt_neu)\n",
    "\n",
    "Pos = [NTroll_Pos, Troll_Pos]\n",
    "Neg = [NTroll_Neg, Troll_Neg]\n",
    "Neu = [NTroll_Neu, Troll_Neu]\n",
    "\n",
    "List = [Pos, Neg, Neu]\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(List)\n",
    "\n",
    "print(\"Statistics: \", stat, \"\\np-Value: \", p, \"\\nDegree of Freedom: \", dof, \"\\nExpected Frequencies: \\n\", expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting of graph of the above data\n",
    "x = np.arange(len(Pos))\n",
    "\n",
    "width = 0.15  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "rects1 = ax.bar(x - width/2, Pos, width, label='Positive', color = '#32cd32') # #32cd32 - Lime Green\n",
    "rects2 = ax.bar(x + width/2, Neu, width, label='Neutral', color = '#fff44f') # #fff44f - Lime Yellow\n",
    "rects3 = ax.bar(x + width*3/2, Neg, width, label='Negative', color = '#FF0000') # #FF0000 - Red\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Troll and Not Troll by sentiment')\n",
    "plt.xticks(x)\n",
    "ax.set_xticklabels(['Non Troll', 'Troll'])\n",
    "ax.legend(fontsize = 10)\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 10),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: To test whether retweet count for troll and non troll tweets are equal\n",
    "\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "\n",
    "# generate two independent samples\n",
    "data1 = newDf1.loc[newDf1['Troll_label'] == 0, 'retweet_count']\n",
    "data2 = newDf1.loc[newDf1['Troll_label'] == 1, 'retweet_count']\n",
    "\n",
    "# compare samples\n",
    "stat, p = ttest_ind(data1, data2)\n",
    "\n",
    "print('t = %.3f, p = %.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category wise retweet count \n",
    "#Objective: To test whether category wise retweet count differ significantly\n",
    "\n",
    "Political_RT = newDf1.loc[newDf1['category_name'] == 'Political', 'retweet_count']\n",
    "GI_RT = newDf1.loc[newDf1['category_name'] == 'Govt Institutions', 'retweet_count']\n",
    "Gaming_RT = newDf1.loc[newDf1['category_name'] == 'Gaming', 'retweet_count']\n",
    "IT_RT = newDf1.loc[newDf1['category_name'] == 'IT', 'retweet_count']\n",
    "Pharma_RT = newDf1.loc[newDf1['category_name'] == 'Pharma', 'retweet_count']\n",
    "Automobile_RT = newDf1.loc[newDf1['category_name'] == 'automobile', 'retweet_count']\n",
    "MC_RT = newDf1.loc[newDf1['category_name'] == 'movie_celebrity', 'retweet_count']\n",
    "MTS_RT = newDf1.loc[newDf1['category_name'] == 'Movie_TVShows', 'retweet_count']\n",
    "Sports_RT = newDf1.loc[newDf1['category_name'] == 'Sports', 'retweet_count']\n",
    "\n",
    "f_oneway(Political_RT, GI_RT, Gaming_RT, IT_RT, Pharma_RT, Automobile_RT, MC_RT, MTS_RT, Sports_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category wise retweet count \n",
    "#Objective: To test whether category wise followers count differ significantly\n",
    "\n",
    "Political_RT1 = newDf1.loc[newDf1['category_name'] == 'Political', 'user.followers_count']\n",
    "GI_RT1 = newDf1.loc[newDf1['category_name'] == 'Govt Institutions', 'user.followers_count']\n",
    "Gaming_RT1 = newDf1.loc[newDf1['category_name'] == 'Gaming', 'user.followers_count']\n",
    "IT_RT1 = newDf1.loc[newDf1['category_name'] == 'IT', 'user.followers_count']\n",
    "Pharma_RT1 = newDf1.loc[newDf1['category_name'] == 'Pharma', 'user.followers_count']\n",
    "Automobile_RT1 = newDf1.loc[newDf1['category_name'] == 'automobile', 'user.followers_count']\n",
    "MC_RT1 = newDf1.loc[newDf1['category_name'] == 'movie_celebrity', 'user.followers_count']\n",
    "MTS_RT1 = newDf1.loc[newDf1['category_name'] == 'Movie_TVShows', 'user.followers_count']\n",
    "Sports_RT1 = newDf1.loc[newDf1['category_name'] == 'Sports', 'user.followers_count']\n",
    "\n",
    "f_oneway(Political_RT1, GI_RT1, Gaming_RT1, IT_RT1, Pharma_RT1, Automobile_RT1, MC_RT1, MTS_RT1, Sports_RT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category wise retweet count \n",
    "#Objective: To test whether category wise favourite count differ significantly\n",
    "\n",
    "Political_RT1 = newDf1.loc[newDf1['category_name'] == 'Political', 'user.favourites_count']\n",
    "GI_RT1 = newDf1.loc[newDf1['category_name'] == 'Govt Institutions', 'user.favourites_count']\n",
    "Gaming_RT1 = newDf1.loc[newDf1['category_name'] == 'Gaming', 'user.favourites_count']\n",
    "IT_RT1 = newDf1.loc[newDf1['category_name'] == 'IT', 'user.favourites_count']\n",
    "Pharma_RT1 = newDf1.loc[newDf1['category_name'] == 'Pharma', 'user.favourites_count']\n",
    "Automobile_RT1 = newDf1.loc[newDf1['category_name'] == 'automobile', 'user.favourites_count']\n",
    "MC_RT1 = newDf1.loc[newDf1['category_name'] == 'movie_celebrity', 'user.favourites_count']\n",
    "MTS_RT1 = newDf1.loc[newDf1['category_name'] == 'Movie_TVShows', 'user.favourites_count']\n",
    "Sports_RT1 = newDf1.loc[newDf1['category_name'] == 'Sports', 'user.favourites_count']\n",
    "\n",
    "f_oneway(Political_RT1, GI_RT1, Gaming_RT1, IT_RT1, Pharma_RT1, Automobile_RT1, MC_RT1, MTS_RT1, Sports_RT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: Chi-Square test of association. To test whether association between trolling and categories\n",
    "\n",
    "df_association = tntcc.iloc[:, :3]\n",
    "\n",
    "Nt = []\n",
    "T = []\n",
    "for nt in df_association['Not Troll']:\n",
    "    Nt.append(nt)\n",
    "\n",
    "for t in df_association['Troll']:\n",
    "    T.append(t)\n",
    "\n",
    "NT_T_list = [Nt, T]\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(NT_T_list)\n",
    "\n",
    "print(\"Statistics: \", stat, \"\\np-Value: \", p, \"\\nDegree of Freedom: \", dof, \"\\nExpected Frequencies: \\n\", expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
